{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYHvmaC8r5ApcjIyV2VKfu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Seleccionar versió correcte tensorflow"],"metadata":{"id":"uALhPkGJUKmc"}},{"cell_type":"code","source":["%tensorflow_version 2.x"],"metadata":{"id":"uh4z9LheW9az"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Clonar projecte GitHub (accés ràpid DataSet)"],"metadata":{"id":"OY9EbETdUWRY"}},{"cell_type":"code","source":["!git clone https://github.com/mesushan/CNN-for-image-Classification.git"],"metadata":{"id":"Bj355rdUXAch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! ls"],"metadata":{"id":"T5Tp25ROZgjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"dBNqmjM2ZhDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iniciar CNN\n","model = tf.keras.models.Sequential()"],"metadata":{"id":"E0AV0q4HZi8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Afegir capa convolucional"],"metadata":{"id":"7ElclEspUuZB"}},{"cell_type":"code","source":["# 32 detectors de característiques amb 3*3 dimensions, així que la capa convolucional esta composta per mapes de 32 característiques\n","# 128 per 128 dimensions amb imatges en color (3 canals) (tensorflow backend)\n","input_size = (128, 128)\n","model.add(tf.keras.layers.Convolution2D(32, 3, 3, input_shape = (*input_size, 3), activation = 'relu'))"],"metadata":{"id":"nL7OdFbxZqpU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Afegir capa de conjunt"],"metadata":{"id":"Q3F9bwysU7AJ"}},{"cell_type":"code","source":["# Reduir la mida dels mapes de característiques i reduir el nombre de nodes a la futura capa totalment connectada (reduir temps de complexitat, \n","# menys càlcul intens sense perdre el rendiment). La dimensió 2 per 2 és l'opció recomanada\n","model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"],"metadata":{"id":"SmdvEO30aFEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Afegir segona capa convolucional amb ampliament"],"metadata":{"id":"IB3dN8oGVIZA"}},{"cell_type":"code","source":["model.add(tf.keras.layers.Convolution2D(32, 3, 3, activation = 'relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"],"metadata":{"id":"CUzS1hW1anYl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6. Afegir capa d'aplanació"],"metadata":{"id":"Jt7Zewy_V1G9"}},{"cell_type":"code","source":["# aplaneu tots els mapes de característiques de la capa d'agrupació en un sol vector\n","model.add(tf.keras.layers.Flatten())"],"metadata":{"id":"kBHJ7x5WapfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Afegir una capa totalment connectada"],"metadata":{"id":"PBj2NpdoWDpk"}},{"cell_type":"code","source":["# making classic ann which compose of fully connected layers\n","# number of nodes in hidden layer (output_dim) (common practice is to take the power of 2)\n","model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n","model.add(tf.keras.layers.Dropout(0.5))\n","model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"],"metadata":{"id":"H2_sWJWla0MB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Compilació del model"],"metadata":{"id":"aPMVivpuWLFh"}},{"cell_type":"code","source":["# Compiling the CNN\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"metadata":{"id":"rOt7IgAzbDfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Ajustar la CNN a les imatges"],"metadata":{"id":"6825XKulWUOk"}},{"cell_type":"code","source":["# image augmentation technique to enrich our dataset(training set) without adding more images so get good performance  results with little or no overfitting even with the small amount of images\n","# used from keras documentation (flow_from_directory method)\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","batch_size = 32\n","# image augmentation part\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","# create training set\n","# wanna get higher accuracy -> inccrease target_size\n","training_set = train_datagen.flow_from_directory('/content/CNN-for-image-Classification/dataset/training_set',\n","                                                 target_size = input_size,\n","                                                 batch_size = batch_size,\n","                                                 class_mode = 'binary')\n","\n","# create test set\n","# wanna get higher accuracy -> inccrease target_size\n","test_set = test_datagen.flow_from_directory('/content/CNN-for-image-Classification/dataset/test_set',\n","                                            target_size = input_size,\n","                                            batch_size = batch_size,\n","                                            class_mode = 'binary')\n","\n","# fit the cnn model to the trainig set and testing it on the test set\n","model.fit(training_set,\n","          steps_per_epoch = 8000/batch_size,\n","          epochs = 35,\n","          validation_data = test_set,\n","          validation_steps = 2000/batch_size)"],"metadata":{"id":"XLvueu_4bEJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Fer noves preddicions"],"metadata":{"id":"nodDmOUAWo8r"}},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing import image"],"metadata":{"id":"bs2MPogabKr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_image = image.load_img('/content/CNN-for-image-Classification/dataset/single_prediction/cat_or_dog_4.jpg', target_size= input_size)\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = model.predict(test_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"BktVa2YabLFx","executionInfo":{"status":"error","timestamp":1684003506977,"user_tz":-120,"elapsed":325,"user":{"displayName":"Ana Torres Ortuño","userId":"10199732357667420804"}},"outputId":"cf618fb5-6d02-41e7-da79-2382efa78cd5"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2f03af39ee77>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/CNN-for-image-Classification/dataset/single_prediction/cat_or_dog_4.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"]}]},{"cell_type":"code","source":["training_set.class_indices"],"metadata":{"id":"Ri0E3qYPbMc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if result [0][0] == 1:\n","  prediction = 'patològic'\n","else:\n","  prediction = 'sa'"],"metadata":{"id":"qyNQeC1FbNy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction"],"metadata":{"id":"HeCInvkGbPPr"},"execution_count":null,"outputs":[]}]}