{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNulAVMHOhRvE1EmcI154Br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anatorres09/DE-MRI_Classificacio_CNN/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connectar a Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIkP_r4_hCNF",
        "outputId": "f777fd0b-964f-4b4e-d361-fdf7ac81d030"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar llibreries\n",
        "import os\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "hB1qkfA2hFrV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_folder = '/content/drive/MyDrive/CNN'"
      ],
      "metadata": {
        "id": "zLyQ52AOhGa2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN IMATGES .PNG\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_normal = os.path.join(drive_folder, \"corr_entrenament\", \"corr_normal\")\n",
        "ruta_carpeta_patologic = os.path.join(drive_folder, \"corr_entrenament\", \"corr_patologic\")\n",
        "\n",
        "resize_shape = (128, 128)\n",
        "\n",
        "# Funció per recollir les imatges i les etiquetes\n",
        "def recollir_imatges_etiquetes(ruta_carpeta, etiqueta, resize_shape):\n",
        "    casos = [nom_carpeta for nom_carpeta in os.listdir(ruta_carpeta) if os.path.isdir(os.path.join(ruta_carpeta, nom_carpeta))]\n",
        "\n",
        "    imatges = []\n",
        "    etiquetes = []\n",
        "\n",
        "    for cas in casos:\n",
        "        ruta_cas = os.path.join(ruta_carpeta, cas)\n",
        "        rutes_imatges = glob(os.path.join(ruta_cas, '*.png'))\n",
        "\n",
        "        \n",
        "\n",
        "        for ruta_imatge in rutes_imatges:\n",
        "            imatge = cv2.imread(ruta_imatge)\n",
        "            imatge = cv2.cvtColor(imatge, cv2.COLOR_BGR2RGB)\n",
        "             # Aplicar la estrategia 1: Redimensionar las imágenes\n",
        "            imatge = cv2.resize(imatge, resize_shape)\n",
        "\n",
        "\n",
        "            imatge = imatge.astype('float16') / 255.0  # Convertir a float16 y normalizar entre 0 y 1\n",
        "            imatges.append(imatge)\n",
        "            etiquetes.append(etiqueta)\n",
        "\n",
        "    return imatges, etiquetes\n",
        "\n",
        "# Recollir imatges i etiquetes de la carpeta corr_normal\n",
        "imatges_normal, etiquetes_normal = recollir_imatges_etiquetes(ruta_carpeta_normal, \"normal\", resize_shape)\n",
        "\n",
        "# Recollir imatges i etiquetes de la carpeta corr_patologic\n",
        "imatges_patologic, etiquetes_patologic = recollir_imatges_etiquetes(ruta_carpeta_patologic, \"patologic\", resize_shape)\n",
        "\n",
        "# Concatenar les llistes de imatges i etiquetes\n",
        "imatges_total = imatges_normal + imatges_patologic\n",
        "etiquetes_total = etiquetes_normal + etiquetes_patologic\n",
        "\n",
        "# Convertir la llista de imatges i etiquetes en una matriu numpy\n",
        "matriu_imatges = np.array(imatges_total, dtype='float16')\n",
        "etiquetes = np.array(etiquetes_total)\n",
        "\n",
        "print(f\"S'han recollit {matriu_imatges.shape[0]} imatges en total.\")\n",
        "\n",
        "# Estrategia 3: Dividir el conjunto de datos en conjuntos de entrenamiento, validación y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(matriu_imatges, etiquetes, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Obtenir la quantitat de imatges etiquetades com \"normal\"\n",
        "quantitat_normal = np.count_nonzero(etiquetes == \"normal\")\n",
        "\n",
        "# Obtenir la quntitat de imatges etiquetades com \"patologic\"\n",
        "quantitat_patologic = np.count_nonzero(etiquetes == \"patologic\")\n",
        "\n",
        "print(f\"S'han trobat {quantitat_normal} imatges etiquetades com 'normal'.\")\n",
        "print(f\"S'han trobat {quantitat_patologic} imatges etiquetades com 'patologic'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6st4DqdFg08Q",
        "outputId": "3fd37753-b939-4ce8-d296-8c5895f16b9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S'han recollit 699 imatges en total.\n",
            "S'han trobat 226 imatges etiquetades com 'normal'.\n",
            "S'han trobat 473 imatges etiquetades com 'patologic'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessat de les imatges\n",
        "# Normalitzar els valors de píxels al rang [0, 1]\n",
        "imatges = matriu_imatges/ 255.0\n",
        "\n",
        "# Dividir les dades en conjunts d'entrenament i prova\n",
        "X_train, X_test, y_train, y_test = train_test_split(imatges, etiquetes, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert encoded labels to one-hot encoding\n",
        "y_train = tf.one_hot(y_train_encoded, depth=2)\n",
        "y_test = tf.one_hot(y_test_encoded, depth=2)\n",
        "\n",
        "# Construir el model de la CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train[0].shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "# Crear generadores de datos para el entrenamiento y la prueba\n",
        "generador_entrenamiento = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.3, dtype='float16')\n",
        "generador_prueba = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.3, dtype='float16')\n",
        "\n",
        "# Crear generadores de flujo de datos para el entrenamiento y la prueba\n",
        "flujo_entrenamiento = generador_entrenamiento.flow(X_train, y_train, batch_size=32)\n",
        "flujo_prueba = generador_prueba.flow(X_test, y_test, batch_size=32)\n",
        "\n",
        "\n",
        "# Antes de entrenar el modelo\n",
        "gc.collect()  # Liberar la memoria antes del entrenamiento\n",
        "\n",
        "# Entrenar el modelo utilizando generadores de datos\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba utilizando generador de datos\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Perdua en el conjunt de prova: {loss}\")\n",
        "print(f\"Precisió en el conjunt de prova: {accuracy}\")\n",
        "\n",
        "# Obtener las predicciones del conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convertir las predicciones en etiquetas codificadas\n",
        "y_pred_encoded = np.argmax(y_pred, axis=1)\n",
        "y_test_encoded = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calcular y mostrar la matriz de confusión\n",
        "confusion_mat = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calcular y mostrar el informe de clasificación\n",
        "classification_rep = classification_report(y_test_encoded, y_pred_encoded)\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_rep)\n",
        "\n",
        "# Calcular y mostrar la precisión\n",
        "precision = precision_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Precisión: {precision}\")\n",
        "\n",
        "# Calcular y mostrar el recall\n",
        "recall = recall_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Calcular y mostrar el F1-score\n",
        "f1 = f1_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Calcular y mostrar la exactitud (accuracy)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Exactitud: {accuracy}\")\n",
        "\n",
        "# Liberar la memoria después de evaluar el modelo\n",
        "del X_train, y_train, X_test, y_test\n",
        "gc.collect()  # Liberar la memoria utilizada por las variables eliminadas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSZP4jZNg2Ai",
        "outputId": "bae537d5-9393-4072-87a6-85479e4cc27a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 2s 33ms/step - loss: 0.6356 - accuracy: 0.6923 - val_loss: 0.6652 - val_accuracy: 0.6143\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.6167 - accuracy: 0.6923 - val_loss: 0.6443 - val_accuracy: 0.6143\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 0.5870 - accuracy: 0.6923 - val_loss: 0.6354 - val_accuracy: 0.6143\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 0s 25ms/step - loss: 0.5574 - accuracy: 0.7156 - val_loss: 0.5691 - val_accuracy: 0.6214\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 0s 24ms/step - loss: 0.5258 - accuracy: 0.7174 - val_loss: 0.5589 - val_accuracy: 0.6786\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.5319 - accuracy: 0.7174 - val_loss: 0.5323 - val_accuracy: 0.7071\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.5095 - accuracy: 0.7352 - val_loss: 0.5283 - val_accuracy: 0.8143\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 0s 21ms/step - loss: 0.5006 - accuracy: 0.7299 - val_loss: 0.5213 - val_accuracy: 0.8286\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.4933 - accuracy: 0.7496 - val_loss: 0.5008 - val_accuracy: 0.8286\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 0s 20ms/step - loss: 0.4770 - accuracy: 0.7639 - val_loss: 0.4911 - val_accuracy: 0.8071\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4911 - accuracy: 0.8071\n",
            "Perdua en el conjunt de prova: 0.4911430776119232\n",
            "Precisió en el conjunt de prova: 0.8071428537368774\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "Matriz de confusión:\n",
            "[[31 23]\n",
            " [ 4 82]]\n",
            "Informe de clasificación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.57      0.70        54\n",
            "           1       0.78      0.95      0.86        86\n",
            "\n",
            "    accuracy                           0.81       140\n",
            "   macro avg       0.83      0.76      0.78       140\n",
            "weighted avg       0.82      0.81      0.80       140\n",
            "\n",
            "Precisión: 0.780952380952381\n",
            "Recall: 0.9534883720930233\n",
            "F1-score: 0.8586387434554973\n",
            "Exactitud: 0.8071428571428572\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3303"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar y preprocesar las imagenes para la predicción\n",
        "\n",
        "# Ruta de la carpeta de imágenes en Drive\n",
        "\n",
        "ruta_carpeta_test = os.path.join(drive_folder, \"test\")\n",
        "\n",
        "# Obtener la lista de nombres de archivos de las imágenes en la carpeta\n",
        "nombres_archivos = os.listdir(ruta_carpeta_test)\n",
        "\n",
        "# Realizar la predicción para cada imagen en la carpeta\n",
        "for nombre_archivo in nombres_archivos:\n",
        "    # Ruta completa de la imagen\n",
        "    ruta_imagen = os.path.join(ruta_carpeta_imagenes, nombre_archivo)\n",
        "\n",
        "    # Cargar y preprocesar la imagen\n",
        "    nueva_imagen = cv2.imread(ruta_imagen)\n",
        "    nueva_imagen = cv2.cvtColor(nueva_imagen, cv2.COLOR_BGR2RGB)\n",
        "    nueva_imagen = cv2.resize(nueva_imagen, resize_shape)\n",
        "    nueva_imagen = nueva_imagen.astype('float32') / 255.0\n",
        "\n",
        "    # Realizar la predicción de la nueva imagen\n",
        "    prediccion = model.predict(np.expand_dims(nueva_imagen, axis=0))\n",
        "    clase_predicha = np.argmax(prediccion)\n",
        "\n",
        "    print(f\"La clase predicha para la imagen {nombre_archivo} es: {clase_predicha}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "RbuKyvN94IrM",
        "outputId": "7f488efd-7a9d-4f15-fc25-636657c39068"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-748f811dc848>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Obtener la lista de nombres de archivos de las imágenes en la carpeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnombres_archivos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_carpeta_imagenes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Realizar la predicción para cada imagen en la carpeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CNN/test'"
          ]
        }
      ]
    }
  ]
}