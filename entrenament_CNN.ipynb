{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsrKkXKasAPHYNTFSOsm9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anatorres09/DE-MRI_Classificacio_CNN/blob/main/entrenament_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connectar a Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "KIkP_r4_hCNF",
        "outputId": "08e27608-5609-4ca4-cd0e-6ea1cc25c389"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-12edb3019eb6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Connectar a Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar llibreries\n",
        "import os\n",
        "from glob import glob\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from joblib import Parallel, delayed"
      ],
      "metadata": {
        "id": "hB1qkfA2hFrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_folder = '/content/drive/MyDrive/CNN'"
      ],
      "metadata": {
        "id": "zLyQ52AOhGa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN IMATGES .PNG\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_normal = os.path.join(drive_folder, \"corr_entrenament\", \"corr_normal\")\n",
        "ruta_carpeta_patologic = os.path.join(drive_folder, \"corr_entrenament\", \"corr_patologic\")\n",
        "# Estrategia 1: Redimensionar las imágenes a un tamaño fijo\n",
        "resize_shape = (128, 128)\n",
        "\n",
        "# Recoger imágenes y etiquetas de la carpeta corr_normal\n",
        "def recoger_imagenes_etiquetas(ruta_carpeta, etiqueta, resize_shape):\n",
        "    casos = [nom_carpeta for nom_carpeta in os.listdir(ruta_carpeta) if os.path.isdir(os.path.join(ruta_carpeta, nom_carpeta))]\n",
        "\n",
        "    imatges = []\n",
        "    etiquetes = []\n",
        "\n",
        "    for cas in casos:\n",
        "        ruta_cas = os.path.join(ruta_carpeta, cas)\n",
        "        rutes_imatges = glob(os.path.join(ruta_cas, '*.png'))\n",
        "\n",
        "        for ruta_imatge in rutes_imatges:\n",
        "            imatge = cv2.imread(ruta_imatge)\n",
        "            imatge = cv2.cvtColor(imatge, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Aplicar la estrategia 1: Redimensionar las imágenes\n",
        "            imatge = cv2.resize(imatge, resize_shape)\n",
        "            \n",
        "            imatge = imatge.astype('float32') / 255.0  # Convertir a float32 y normalizar entre 0 y 1\n",
        "            imatges.append(imatge)\n",
        "            etiquetes.append(etiqueta)\n",
        "\n",
        "    return imatges, etiquetes\n",
        "\n",
        "\n",
        "\n",
        "# Recollir imatges i etiquetes de la carpeta corr_normal\n",
        "imatges_normal, etiquetes_normal = recollir_imatges_etiquetes(ruta_carpeta_normal, \"normal\", resize_shape)\n",
        "\n",
        "# Recollir imatges i etiquetes de la carpeta corr_patologic\n",
        "imatges_patologic, etiquetes_patologic = recollir_imatges_etiquetes(ruta_carpeta_patologic, \"patologic\", resize_shape)\n",
        "\n",
        "# Concatenar les llistes de imatges i etiquetes\n",
        "imatges_total = imatges_normal + imatges_patologic\n",
        "etiquetes_total = etiquetes_normal + etiquetes_patologic\n",
        "\n",
        "# Convertir la lista de imágenes y etiquetas en una matriz numpy\n",
        "matriu_imatges = np.array(imatges_total, dtype='float32')\n",
        "etiquetes = np.array(etiquetes_total)\n",
        "\n",
        "print(f\"S'han recollit {matriu_imatges.shape[0]} imatges en total.\")\n",
        "\n",
        "\n",
        "# Obtenir la quantitat de imatges etiquetades com \"normal\"\n",
        "quantitat_normal = np.count_nonzero(etiquetes == \"normal\")\n",
        "\n",
        "# Obtenir la quntitat de imatges etiquetades com \"patologic\"\n",
        "quantitat_patologic = np.count_nonzero(etiquetes == \"patologic\")\n",
        "\n",
        "print(f\"S'han trobat {quantitat_normal} imatges etiquetades com 'normal'.\")\n",
        "print(f\"S'han trobat {quantitat_patologic} imatges etiquetades com 'patologic'.\")"
      ],
      "metadata": {
        "id": "6st4DqdFg08Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessat de les imatges\n",
        "# Normalitzar els valors de píxels al rang [0, 1]\n",
        "imatges = matriu_imatges/ 255.0\n",
        "\n",
        "# Dividir les dades en conjunts d'entrenament i prova\n",
        "X_train, X_test, y_train, y_test = train_test_split(imatges, etiquetes, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert encoded labels to one-hot encoding\n",
        "y_train = tf.one_hot(y_train_encoded, depth=2)\n",
        "y_test = tf.one_hot(y_test_encoded, depth=2)\n",
        "\n",
        "# Construir el model de la CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train[0].shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "# Crear generadores de datos para el entrenamiento y la prueba\n",
        "generador_entrenamiento = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.3, dtype='float16')\n",
        "generador_prueba = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.3, dtype='float16')\n",
        "\n",
        "# Crear generadores de flujo de datos para el entrenamiento y la prueba\n",
        "flujo_entrenamiento = generador_entrenamiento.flow(X_train, y_train, batch_size=32)\n",
        "flujo_prueba = generador_prueba.flow(X_test, y_test, batch_size=32)\n",
        "\n",
        "\n",
        "# Antes de entrenar el modelo\n",
        "gc.collect()  # Liberar la memoria antes del entrenamiento\n",
        "\n",
        "# Entrenar el modelo utilizando generadores de datos\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), class_weight={\"normal\": 1.54, \"patologic\": 0.74}.)\n",
        "\n",
        "\n",
        "#Ruta del archivo donde se guardará el modelo\n",
        "ruta_model = os.path.join(drive_folder, \"model_CNN.h5\")\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(ruta_model)\n",
        "print(\"Model de la CNN guardat correctament.\")\n",
        "\n",
        "# Evaluar el modelo en el conjunto de prueba utilizando generador de datos\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Perdua en el conjunt de prova: {loss}\")\n",
        "print(f\"Precisió en el conjunt de prova: {accuracy}\")\n",
        "\n",
        "# Obtener las predicciones del conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convertir las predicciones en etiquetas codificadas\n",
        "y_pred_encoded = np.argmax(y_pred, axis=1)\n",
        "y_test_encoded = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calcular y mostrar la matriz de confusión\n",
        "confusion_mat = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calcular y mostrar el informe de clasificación\n",
        "classification_rep = classification_report(y_test_encoded, y_pred_encoded)\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_rep)\n",
        "\n",
        "# Calcular y mostrar la precisión\n",
        "precision = precision_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Precisión: {precision}\")\n",
        "\n",
        "# Calcular y mostrar el recall\n",
        "recall = recall_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Calcular y mostrar el F1-score\n",
        "f1 = f1_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Calcular y mostrar la exactitud (accuracy)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "print(f\"Exactitud: {accuracy}\")\n",
        "\n",
        "# Liberar la memoria después de evaluar el modelo\n",
        "del X_train, y_train, X_test, y_test\n",
        "gc.collect()  # Liberar la memoria utilizada por las variables eliminadas"
      ],
      "metadata": {
        "id": "vSZP4jZNg2Ai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}