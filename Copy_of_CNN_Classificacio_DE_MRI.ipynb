{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anatorres09/DE-MRI_Classificacio_CNN/blob/main/Copy_of_CNN_Classificacio_DE_MRI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Seleccionar versió correcte tensorflow"
      ],
      "metadata": {
        "id": "uALhPkGJUKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x"
      ],
      "metadata": {
        "id": "uh4z9LheW9az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a0c400-334e-4e4d-b68a-fad945485767"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clonar projecte GitHub (accés ràpid DataSet)"
      ],
      "metadata": {
        "id": "OY9EbETdUWRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anatorres09/DE-MRI_Classificacio_CNN.git"
      ],
      "metadata": {
        "id": "Bj355rdUXAch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8f72e5-ab13-4850-c2eb-12a17006df93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DE-MRI_Classificacio_CNN'...\n",
            "remote: Enumerating objects: 919, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 919 (delta 11), reused 57 (delta 11), pack-reused 862\u001b[K\n",
            "Receiving objects: 100% (919/919), 166.34 MiB | 13.38 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Updating files: 100% (153/153), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "T5Tp25ROZgjX",
        "outputId": "aa1ad65f-267f-4bda-a0cc-b7232b109a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DE-MRI_Classificacio_CNN  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "dBNqmjM2ZhDM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar CNN\n",
        "model = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "E0AV0q4HZi8j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Afegir capa convolucional"
      ],
      "metadata": {
        "id": "7ElclEspUuZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 detectors de característiques amb 3*3 dimensions, així que la capa convolucional esta composta per mapes de 32 característiques\n",
        "# 128 per 128 dimensions amb imatges en color (3 canals) (tensorflow backend)\n",
        "input_size = (128, 128)\n",
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, input_shape = (*input_size, 3), activation = 'relu'))"
      ],
      "metadata": {
        "id": "nL7OdFbxZqpU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Afegir capa de conjunt"
      ],
      "metadata": {
        "id": "Q3F9bwysU7AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduir la mida dels mapes de característiques i reduir el nombre de nodes a la futura capa totalment connectada (reduir temps de complexitat, \n",
        "# menys càlcul intens sense perdre el rendiment). La dimensió 2 per 2 és l'opció recomanada\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"
      ],
      "metadata": {
        "id": "SmdvEO30aFEh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Afegir segona capa convolucional amb ampliament"
      ],
      "metadata": {
        "id": "IB3dN8oGVIZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"
      ],
      "metadata": {
        "id": "CUzS1hW1anYl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Afegir capa d'aplanació"
      ],
      "metadata": {
        "id": "Jt7Zewy_V1G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aplaneu tots els mapes de característiques de la capa d'agrupació en un sol vector\n",
        "model.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "kBHJ7x5WapfU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Afegir una capa totalment connectada"
      ],
      "metadata": {
        "id": "PBj2NpdoWDpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making classic ann which compose of fully connected layers\n",
        "# number of nodes in hidden layer (output_dim) (common practice is to take the power of 2)\n",
        "model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "H2_sWJWla0MB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Compilació del model"
      ],
      "metadata": {
        "id": "aPMVivpuWLFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the CNN\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "rOt7IgAzbDfU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Ajustar la CNN a les imatges"
      ],
      "metadata": {
        "id": "6825XKulWUOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llibreries\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "# Directorio donde se encuentran las imágenes en formato NIfTI\n",
        "nifti_dir = 'dataset/entrenament/normal'\n",
        "\n",
        "\n",
        "# Obtener la lista de archivos en el directorio y ordenarla alfabéticamente\n",
        "lista_archivos = sorted(os.listdir(nifti_dir))\n",
        "\n",
        "#Especifica las dimensiones de una imagen\n",
        "dimensiones = (128, 128) + (len(lista_archivos),)\n",
        "\n",
        "# Crear un arreglo NumPy vacío para almacenar todas las imágenes\n",
        "arreglo_imagenes = np.zeros(dimensiones, dtype=np.float16) #modificar el float si es necesario (32,64...)\n",
        "\n",
        "# Cargar todas las imágenes en el arreglo\n",
        "for i, archivo in enumerate(lista_archivos):\n",
        "    imagen = nib.load(os.path.join(nifti_dir, archivo)).get_fdata()\n",
        "    arreglo_imagenes[:, :, i] = imagen\n",
        "arreglo_imagenes = np.transpose(arreglo_imagenes, (2, 0, 1))\n",
        "\n",
        "# Guardar el arreglo de imágenes en formato NumPy\n",
        "np.save('dataset/entrenament/normal/prueba.npy', arreglo_imagenes)\n",
        "\n",
        "\n",
        "# image augmentation technique to enrich our dataset(training set) without adding more images so get good performance  results with little or no overfitting even with the small amount of images\n",
        "# used from keras documentation (flow_from_directory method)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 32\n",
        "# image augmentation part\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# create training set\n",
        "# wanna get higher accuracy -> inccrease target_size\n",
        "training_set = train_datagen.flow_from_directory('dataset/entrenament/normal/prueba.npy', \n",
        "                                                 target_size = input_size,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "# create test set\n",
        "# wanna get higher accuracy -> inccrease target_size\n",
        "test_set = test_datagen.flow_from_directory('dataset/entrenament/normal/prueba.npy',\n",
        "                                            target_size = input_size,\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# fit the cnn model to the trainig set and testing it on the test set\n",
        "model.fit(training_set,\n",
        "          steps_per_epoch = 8000/batch_size,\n",
        "          epochs = 35,\n",
        "          validation_data = test_set,\n",
        "          validation_steps = 2000/batch_size)"
      ],
      "metadata": {
        "id": "XLvueu_4bEJh",
        "outputId": "240008ec-a101-4f29-fa4e-e87496a673e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2599c41a88c5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Obtener la lista de archivos en el directorio y ordenarla alfabéticamente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlista_archivos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnifti_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Especifica las dimensiones de una imagen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/entrenament/normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Fer noves preddicions"
      ],
      "metadata": {
        "id": "nodDmOUAWo8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "bs2MPogabKr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = image.load_img('C:/Users/anato/OneDrive/Escritorio/Entrenament/prueba.npy', target_size= input_size)\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model.predict(test_image)"
      ],
      "metadata": {
        "id": "BktVa2YabLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.class_indices"
      ],
      "metadata": {
        "id": "Ri0E3qYPbMc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if result [0][0] == 1:\n",
        "  prediction = 'patològic'\n",
        "else:\n",
        "  prediction = 'sa'"
      ],
      "metadata": {
        "id": "qyNQeC1FbNy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "id": "HeCInvkGbPPr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}